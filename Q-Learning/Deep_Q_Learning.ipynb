{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 5\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"State space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 300\n",
    "test_episodes = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "def agent(state_shape, action_shape):\n",
    "    \"\"\"The agent maps X-states to Y-actions, where X-states are the inputs and represent\n",
    "        the states on the grid. Y-actions are the outputs and represent the Q-Value of each action\"\"\"\n",
    "    # Just build a keras NN with specific inputs - outputs shapes\n",
    "    init = tf.keras.initializers.HeUniform()\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
    "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
    "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_QS(model, state, step):\n",
    "    return model.predict(state.reshape([1, state.shape[0]]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_learning_rate = 0.7\n",
    "discount_factor = 0.618\n",
    "\n",
    "def train(env, replay_memory, model, target_model, done):\n",
    "    \"\"\"\n",
    "    This updates the model using the bellman equation\n",
    "    \"\"\"\n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    batch_size = 64 * 2\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    current_states = np.array([transition[0] for transition in mini_batch])\n",
    "    current_qs_list = model.predict(current_states)\n",
    "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "    future_qs_list = target_model.predict(new_current_states)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - agent_learning_rate) * current_qs[action] + agent_learning_rate * max_future_q\n",
    "\n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    epsilon = 1 # Epsilon greedy algorithm\n",
    "    max_epsilon = 1\n",
    "    min_epsilon = 0.01\n",
    "    decay = 0.01\n",
    "    print(\"observation space: \", env.observation_space.shape)\n",
    "    model = agent(env.observation_space.shape, env.action_space.n)\n",
    "    # target model is updated once every 100 steps\n",
    "    target_model = agent(env.observation_space.shape, env.action_space.n)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    replay_memory = deque(maxlen=50_000)\n",
    "    target_update_counter = 0\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    steps_to_update_target_model = 0\n",
    "    for episode in range(train_episodes):\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (done or truncated):\n",
    "            steps_to_update_target_model += 1\n",
    "            if True:\n",
    "                env.render()\n",
    "\n",
    "            random_number = np.random.rand()\n",
    "\n",
    "            if random_number <= epsilon:\n",
    "                # explore\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # exploit\n",
    "                encoded = observation[0] if isinstance(observation, tuple) else observation\n",
    "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
    "                predicted = model.predict(encoded_reshaped).flatten()\n",
    "                action = np.argmax(predicted)\n",
    "            new_observation, reward, done, truncated, info = env.step(action)\n",
    "             \n",
    "            replay_memory.append([observation[0] if isinstance(observation, tuple) else observation, action, reward, new_observation, done])\n",
    "\n",
    "            if steps_to_update_target_model % 4 == 0 or done:\n",
    "                train(env, replay_memory, model, target_model, done)\n",
    "\n",
    "            observation = new_observation\n",
    "            total_training_rewards += reward\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Total training rewards: {total_training_rewards}, after n steps = {episode}, with final reward = {reward}\")\n",
    "                total_training_rewards += 1\n",
    "\n",
    "                if steps_to_update_target_model >= 100:\n",
    "                    print(\"Copying main network weights  to the target network weights\")\n",
    "                    target_model.set_weights(model.get_weights())\n",
    "                    steps_to_update_target_model = 0\n",
    "                break\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode) # interpolate\n",
    "    env.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  (4,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hitchhiker\\.conda\\envs\\tf-gpuu\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:212: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  \"You are calling render method without specifying any render mode. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training rewards: 13.0, after n steps = 0, with final reward = 1.0\n",
      "Total training rewards: 20.0, after n steps = 1, with final reward = 1.0\n",
      "Total training rewards: 30.0, after n steps = 2, with final reward = 1.0\n",
      "Total training rewards: 17.0, after n steps = 3, with final reward = 1.0\n",
      "Total training rewards: 17.0, after n steps = 4, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 5, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 46.0, after n steps = 6, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 7, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 8, with final reward = 1.0\n",
      "Total training rewards: 28.0, after n steps = 9, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 25.0, after n steps = 10, with final reward = 1.0\n",
      "Total training rewards: 24.0, after n steps = 11, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 12, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 13, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 14, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 15, with final reward = 1.0\n",
      "Total training rewards: 18.0, after n steps = 16, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 24.0, after n steps = 17, with final reward = 1.0\n",
      "Total training rewards: 23.0, after n steps = 18, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 19, with final reward = 1.0\n",
      "Total training rewards: 27.0, after n steps = 20, with final reward = 1.0\n",
      "Total training rewards: 24.0, after n steps = 21, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 35.0, after n steps = 22, with final reward = 1.0\n",
      "Total training rewards: 19.0, after n steps = 23, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 24, with final reward = 1.0\n",
      "Total training rewards: 18.0, after n steps = 25, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 26, with final reward = 1.0\n",
      "Total training rewards: 26.0, after n steps = 27, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 12.0, after n steps = 28, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 29, with final reward = 1.0\n",
      "Total training rewards: 18.0, after n steps = 30, with final reward = 1.0\n",
      "Total training rewards: 21.0, after n steps = 31, with final reward = 1.0\n",
      "Total training rewards: 40.0, after n steps = 32, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 14.0, after n steps = 33, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 34, with final reward = 1.0\n",
      "Total training rewards: 20.0, after n steps = 35, with final reward = 1.0\n",
      "Total training rewards: 19.0, after n steps = 36, with final reward = 1.0\n",
      "Total training rewards: 27.0, after n steps = 37, with final reward = 1.0\n",
      "Total training rewards: 20.0, after n steps = 38, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 14.0, after n steps = 39, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 40, with final reward = 1.0\n",
      "Total training rewards: 15.0, after n steps = 41, with final reward = 1.0\n",
      "Total training rewards: 22.0, after n steps = 42, with final reward = 1.0\n",
      "Total training rewards: 25.0, after n steps = 43, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 44, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 20.0, after n steps = 45, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 46, with final reward = 1.0\n",
      "Total training rewards: 32.0, after n steps = 47, with final reward = 1.0\n",
      "Total training rewards: 18.0, after n steps = 48, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 49, with final reward = 1.0\n",
      "Total training rewards: 52.0, after n steps = 50, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 10.0, after n steps = 51, with final reward = 1.0\n",
      "Total training rewards: 27.0, after n steps = 52, with final reward = 1.0\n",
      "Total training rewards: 19.0, after n steps = 53, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 54, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 55, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 56, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 57, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 19.0, after n steps = 58, with final reward = 1.0\n",
      "Total training rewards: 27.0, after n steps = 59, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 60, with final reward = 1.0\n",
      "Total training rewards: 15.0, after n steps = 61, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 62, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 63, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 64, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 13.0, after n steps = 65, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 66, with final reward = 1.0\n",
      "Total training rewards: 15.0, after n steps = 67, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 68, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 69, with final reward = 1.0\n",
      "Total training rewards: 26.0, after n steps = 70, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 71, with final reward = 1.0\n",
      "Total training rewards: 16.0, after n steps = 72, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 11.0, after n steps = 73, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 74, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 75, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 76, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 77, with final reward = 1.0\n",
      "Total training rewards: 15.0, after n steps = 78, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 79, with final reward = 1.0\n",
      "Total training rewards: 17.0, after n steps = 80, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 10.0, after n steps = 81, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 82, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 83, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 84, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 85, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 86, with final reward = 1.0\n",
      "Total training rewards: 15.0, after n steps = 87, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 88, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 89, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 10.0, after n steps = 90, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 91, with final reward = 1.0\n",
      "Total training rewards: 15.0, after n steps = 92, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 93, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 94, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 95, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 96, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 97, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 98, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 13.0, after n steps = 99, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 100, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 101, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 102, with final reward = 1.0\n",
      "Total training rewards: 8.0, after n steps = 103, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 104, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 105, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 106, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 107, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 108, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 10.0, after n steps = 109, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 110, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 111, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 112, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 113, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 114, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 115, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 116, with final reward = 1.0\n",
      "Total training rewards: 17.0, after n steps = 117, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 12.0, after n steps = 118, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 119, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 120, with final reward = 1.0\n",
      "Total training rewards: 16.0, after n steps = 121, with final reward = 1.0\n",
      "Total training rewards: 18.0, after n steps = 122, with final reward = 1.0\n",
      "Total training rewards: 17.0, after n steps = 123, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 124, with final reward = 1.0\n",
      "Total training rewards: 18.0, after n steps = 125, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 12.0, after n steps = 126, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 127, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 128, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 129, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 130, with final reward = 1.0\n",
      "Total training rewards: 8.0, after n steps = 131, with final reward = 1.0\n",
      "Total training rewards: 8.0, after n steps = 132, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 133, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 134, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 135, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 13.0, after n steps = 136, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 137, with final reward = 1.0\n",
      "Total training rewards: 8.0, after n steps = 138, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 139, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 140, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 141, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 142, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 143, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 144, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 145, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 11.0, after n steps = 146, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 147, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 148, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 149, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 150, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 151, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 152, with final reward = 1.0\n",
      "Total training rewards: 8.0, after n steps = 153, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 154, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 155, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 156, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 10.0, after n steps = 157, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 158, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 159, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 160, with final reward = 1.0\n",
      "Total training rewards: 26.0, after n steps = 161, with final reward = 1.0\n",
      "Total training rewards: 27.0, after n steps = 162, with final reward = 1.0\n",
      "Total training rewards: 28.0, after n steps = 163, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 11.0, after n steps = 164, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 165, with final reward = 1.0\n",
      "Total training rewards: 14.0, after n steps = 166, with final reward = 1.0\n",
      "Total training rewards: 13.0, after n steps = 167, with final reward = 1.0\n",
      "Total training rewards: 11.0, after n steps = 168, with final reward = 1.0\n",
      "Total training rewards: 26.0, after n steps = 169, with final reward = 1.0\n",
      "Total training rewards: 33.0, after n steps = 170, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 30.0, after n steps = 171, with final reward = 1.0\n",
      "Total training rewards: 12.0, after n steps = 172, with final reward = 1.0\n",
      "Total training rewards: 15.0, after n steps = 173, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 174, with final reward = 1.0\n",
      "Total training rewards: 29.0, after n steps = 175, with final reward = 1.0\n",
      "Total training rewards: 8.0, after n steps = 176, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 9.0, after n steps = 177, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 178, with final reward = 1.0\n",
      "Total training rewards: 19.0, after n steps = 179, with final reward = 1.0\n",
      "Total training rewards: 10.0, after n steps = 180, with final reward = 1.0\n",
      "Total training rewards: 32.0, after n steps = 181, with final reward = 1.0\n",
      "Total training rewards: 41.0, after n steps = 182, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 24.0, after n steps = 183, with final reward = 1.0\n",
      "Total training rewards: 30.0, after n steps = 184, with final reward = 1.0\n",
      "Total training rewards: 31.0, after n steps = 185, with final reward = 1.0\n",
      "Total training rewards: 9.0, after n steps = 186, with final reward = 1.0\n",
      "Total training rewards: 49.0, after n steps = 187, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 30.0, after n steps = 188, with final reward = 1.0\n",
      "Total training rewards: 36.0, after n steps = 189, with final reward = 1.0\n",
      "Total training rewards: 21.0, after n steps = 190, with final reward = 1.0\n",
      "Total training rewards: 30.0, after n steps = 191, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 23.0, after n steps = 192, with final reward = 1.0\n",
      "Total training rewards: 69.0, after n steps = 193, with final reward = 1.0\n",
      "Total training rewards: 36.0, after n steps = 194, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 22.0, after n steps = 195, with final reward = 1.0\n",
      "Total training rewards: 32.0, after n steps = 196, with final reward = 1.0\n",
      "Total training rewards: 32.0, after n steps = 197, with final reward = 1.0\n",
      "Total training rewards: 53.0, after n steps = 198, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 29.0, after n steps = 199, with final reward = 1.0\n",
      "Total training rewards: 57.0, after n steps = 200, with final reward = 1.0\n",
      "Total training rewards: 42.0, after n steps = 201, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 23.0, after n steps = 202, with final reward = 1.0\n",
      "Total training rewards: 76.0, after n steps = 203, with final reward = 1.0\n",
      "Total training rewards: 32.0, after n steps = 204, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 20.0, after n steps = 205, with final reward = 1.0\n",
      "Total training rewards: 28.0, after n steps = 206, with final reward = 1.0\n",
      "Total training rewards: 36.0, after n steps = 207, with final reward = 1.0\n",
      "Total training rewards: 26.0, after n steps = 208, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 28.0, after n steps = 209, with final reward = 1.0\n",
      "Total training rewards: 103.0, after n steps = 210, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 62.0, after n steps = 211, with final reward = 1.0\n",
      "Total training rewards: 40.0, after n steps = 212, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 121.0, after n steps = 213, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 146.0, after n steps = 214, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 45.0, after n steps = 215, with final reward = 1.0\n",
      "Total training rewards: 87.0, after n steps = 216, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 217.0, after n steps = 217, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 89.0, after n steps = 218, with final reward = 1.0\n",
      "Total training rewards: 209.0, after n steps = 219, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 78.0, after n steps = 220, with final reward = 1.0\n",
      "Total training rewards: 28.0, after n steps = 221, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 77.0, after n steps = 222, with final reward = 1.0\n",
      "Total training rewards: 45.0, after n steps = 223, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 109.0, after n steps = 224, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 94.0, after n steps = 225, with final reward = 1.0\n",
      "Total training rewards: 79.0, after n steps = 226, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 178.0, after n steps = 227, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 99.0, after n steps = 228, with final reward = 1.0\n",
      "Total training rewards: 22.0, after n steps = 229, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 93.0, after n steps = 230, with final reward = 1.0\n",
      "Total training rewards: 105.0, after n steps = 231, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 87.0, after n steps = 232, with final reward = 1.0\n",
      "Total training rewards: 55.0, after n steps = 233, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 118.0, after n steps = 234, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 151.0, after n steps = 235, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 22.0, after n steps = 236, with final reward = 1.0\n",
      "Total training rewards: 101.0, after n steps = 237, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 128.0, after n steps = 238, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 164.0, after n steps = 239, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 160.0, after n steps = 240, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 83.0, after n steps = 241, with final reward = 1.0\n",
      "Total training rewards: 120.0, after n steps = 242, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 195.0, after n steps = 243, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 150.0, after n steps = 244, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 268.0, after n steps = 245, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 156.0, after n steps = 246, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 59.0, after n steps = 247, with final reward = 1.0\n",
      "Total training rewards: 104.0, after n steps = 248, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 181.0, after n steps = 249, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 280.0, after n steps = 250, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 129.0, after n steps = 251, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 168.0, after n steps = 252, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 72.0, after n steps = 253, with final reward = 1.0\n",
      "Total training rewards: 200.0, after n steps = 254, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 124.0, after n steps = 255, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 85.0, after n steps = 256, with final reward = 1.0\n",
      "Total training rewards: 96.0, after n steps = 257, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 126.0, after n steps = 258, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 139.0, after n steps = 259, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 163.0, after n steps = 260, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 208.0, after n steps = 261, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 118.0, after n steps = 262, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 88.0, after n steps = 263, with final reward = 1.0\n",
      "Total training rewards: 193.0, after n steps = 264, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 188.0, after n steps = 265, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 75.0, after n steps = 266, with final reward = 1.0\n",
      "Total training rewards: 166.0, after n steps = 267, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 159.0, after n steps = 268, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 37.0, after n steps = 269, with final reward = 1.0\n",
      "Total training rewards: 88.0, after n steps = 270, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 125.0, after n steps = 271, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 125.0, after n steps = 272, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 47.0, after n steps = 273, with final reward = 1.0\n",
      "Total training rewards: 132.0, after n steps = 274, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 107.0, after n steps = 275, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 79.0, after n steps = 276, with final reward = 1.0\n",
      "Total training rewards: 93.0, after n steps = 277, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 105.0, after n steps = 278, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 98.0, after n steps = 279, with final reward = 1.0\n",
      "Total training rewards: 87.0, after n steps = 280, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 159.0, after n steps = 281, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 38.0, after n steps = 282, with final reward = 1.0\n",
      "Total training rewards: 27.0, after n steps = 283, with final reward = 1.0\n",
      "Total training rewards: 63.0, after n steps = 284, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 122.0, after n steps = 285, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 78.0, after n steps = 286, with final reward = 1.0\n",
      "Total training rewards: 130.0, after n steps = 287, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 120.0, after n steps = 288, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 150.0, after n steps = 289, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 83.0, after n steps = 290, with final reward = 1.0\n",
      "Total training rewards: 117.0, after n steps = 291, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 155.0, after n steps = 292, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 155.0, after n steps = 293, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 100.0, after n steps = 294, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 35.0, after n steps = 295, with final reward = 1.0\n",
      "Total training rewards: 37.0, after n steps = 296, with final reward = 1.0\n",
      "Total training rewards: 32.0, after n steps = 297, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n",
      "Total training rewards: 97.0, after n steps = 298, with final reward = 1.0\n",
      "Total training rewards: 107.0, after n steps = 299, with final reward = 1.0\n",
      "Copying main network weights  to the target network weights\n"
     ]
    }
   ],
   "source": [
    "my_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 0\n",
      "(4,)\n",
      "step is 1\n",
      "(4,)\n",
      "step is 1\n",
      "Episode reward: 114.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "def watch_agent(model, env, eps):\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    s, info = env.reset()\n",
    "    while not (done or truncated):\n",
    "        theInput = np.asarray(s, dtype=\"float32\")\n",
    "        print(theInput.shape)\n",
    "        a = np.argmax(get_QS(model, s, 0))\n",
    "        print('step is', a)\n",
    "        s, r, done, truncated, info = env.step(a)\n",
    "        episode_reward += r\n",
    "    print(\"Episode reward:\", episode_reward)\n",
    "\n",
    "watch_agent(my_model, env, eps=0)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf-gpuu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c5533ccb158af464ef7140ab42d3d3a6db611e33de62f1732b95b466716ce5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
