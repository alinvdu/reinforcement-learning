{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.kernel_approximation import RBFSampler, Nystroem\n",
    "\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(model, s, eps=0.1):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        values = model.predict_all_actions(s)\n",
    "        return np.argmax(values)\n",
    "    else:\n",
    "        return model.env.action_space.sample()\n",
    "\n",
    "def gather_samples(env, n_episodes=10000):\n",
    "    samples = []\n",
    "    for _ in range(n_episodes):\n",
    "        s, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):\n",
    "            a = env.action_space.sample()\n",
    "            sa = np.concatenate((s, [a]))\n",
    "            samples.append(sa)\n",
    "\n",
    "            s, r, done, truncated, info = env.step(a)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        samples = gather_samples(env)\n",
    "        #self.featurizer = RBFSampler()\n",
    "        self.featurizer = Nystroem()\n",
    "        self.featurizer.fit(samples)\n",
    "        dims = self.featurizer.n_components\n",
    "\n",
    "        self.w = np.zeros(dims)\n",
    "\n",
    "    def predict(self, s, a):\n",
    "        sa = np.concatenate((s, [a]))\n",
    "        x = self.featurizer.transform([sa])[0]\n",
    "        return x @ self.w\n",
    "\n",
    "    def predict_all_actions(self, s):\n",
    "        return [self.predict(s, a) for a in range(self.env.action_space.n)]\n",
    "\n",
    "    def grad(self, s, a):\n",
    "        sa = np.concatenate((s, [a]))\n",
    "        x = self.featurizer.transform([sa])[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model, env, n_episodes=20):\n",
    "    reward_per_episode = np.zeros(n_episodes)\n",
    "    for it in range(n_episodes):\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        s, info = env.reset()\n",
    "        while not (done or truncated):\n",
    "            a = epsilon_greedy(model, s, eps=0)\n",
    "            s, r, done, truncated, info = env.step(a)\n",
    "            episode_reward += r\n",
    "        reward_per_episode[it] = episode_reward\n",
    "    return np.mean(reward_per_episode)\n",
    "\n",
    "def watch_agent(model, env, eps):\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    s, info = env.reset()\n",
    "    while not (done or truncated):\n",
    "        a = epsilon_greedy(model, s, eps=eps)\n",
    "        s, r, done, truncated, info = env.step(a)\n",
    "        episode_reward += r\n",
    "    print(\"Episode reward:\", episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 9.0\n",
      "Epissode: 50, Reward: 24.0\n",
      "Epissode: 100, Reward: 27.0\n",
      "Epissode: 150, Reward: 39.0\n",
      "Epissode: 200, Reward: 45.0\n",
      "Epissode: 250, Reward: 78.0\n",
      "Epissode: 300, Reward: 62.0\n",
      "Epissode: 350, Reward: 53.0\n",
      "Epissode: 400, Reward: 104.0\n",
      "Epissode: 450, Reward: 141.0\n",
      "Epissode: 500, Reward: 32.0\n",
      "Epissode: 550, Reward: 69.0\n",
      "Epissode: 600, Reward: 19.0\n",
      "Epissode: 650, Reward: 14.0\n",
      "Epissode: 700, Reward: 16.0\n",
      "Epissode: 750, Reward: 18.0\n",
      "Epissode: 800, Reward: 77.0\n",
      "Epissode: 850, Reward: 275.0\n",
      "Epissode: 900, Reward: 500.0\n",
      "Epissode: 950, Reward: 349.0\n",
      "Epissode: 1000, Reward: 386.0\n",
      "Epissode: 1050, Reward: 339.0\n",
      "Epissode: 1100, Reward: 396.0\n",
      "Epissode: 1150, Reward: 344.0\n",
      "Epissode: 1200, Reward: 358.0\n",
      "Epissode: 1250, Reward: 358.0\n",
      "Epissode: 1300, Reward: 339.0\n",
      "Epissode: 1350, Reward: 87.0\n",
      "Epissode: 1400, Reward: 11.0\n",
      "Epissode: 1450, Reward: 177.0\n",
      "Epissode: 1500, Reward: 500.0\n",
      "Epissode: 1550, Reward: 500.0\n",
      "Epissode: 1600, Reward: 500.0\n",
      "Epissode: 1650, Reward: 500.0\n",
      "Epissode: 1700, Reward: 500.0\n",
      "Epissode: 1750, Reward: 500.0\n",
      "Epissode: 1800, Reward: 500.0\n",
      "Epissode: 1850, Reward: 500.0\n",
      "Epissode: 1900, Reward: 500.0\n",
      "Epissode: 1950, Reward: 500.0\n",
      "Epissode: 2000, Reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "model = Model(env)\n",
    "reward_per_episode = []\n",
    "\n",
    "watch_agent(model, env, eps=0)\n",
    "\n",
    "n_episodes = 2500\n",
    "for it in range(n_episodes):\n",
    "    s, info  = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    while not (done or truncated):\n",
    "        a = epsilon_greedy(model, s)\n",
    "        s2, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        if done:\n",
    "            target = r\n",
    "        else:\n",
    "            values = model.predict_all_actions(s2)\n",
    "            target = r + GAMMA * np.max(values)\n",
    "\n",
    "        # update the model\n",
    "        g = model.grad(s, a)\n",
    "        err = target - model.predict(s, a)\n",
    "        model.w += ALPHA * err * g\n",
    "\n",
    "        episode_reward += r\n",
    "        s = s2\n",
    "\n",
    "    if (it + 1) % 50 == 0:\n",
    "        print(f\"Epissode: {it + 1}, Reward: {episode_reward}\")\n",
    "\n",
    "    if it > 20 and np.mean(reward_per_episode[-20:]) == 200:\n",
    "        print(\"Early exit\")\n",
    "        break\n",
    "\n",
    "    reward_per_episode.append(episode_reward)\n",
    "\n",
    "test_reward = test_agent(model, env)\n",
    "print(f\"Average test reward: {test_reward}\")\n",
    "\n",
    "plt.plot(reward_per_episode)\n",
    "plt.title(\"Reward per episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: 93.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "watch_agent(model, env, eps=0)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
